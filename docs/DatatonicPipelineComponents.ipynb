{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c600a7a-b0be-4747-9adc-778929574591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dtpc\n",
    "# !pip install datatonic-pipeline-components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28dcbf0c-9218-43ac-9dee-697912730084",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 65\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;129m@dsl\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer_component\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\n\u001b[1;32m     33\u001b[0m     train_data: Input[Dataset],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m     hparams: \u001b[38;5;28mdict\u001b[39m,\n\u001b[1;32m     40\u001b[0m ):\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dsl\u001b[38;5;241m.\u001b[39mContainerSpec(\n\u001b[1;32m     42\u001b[0m         image\u001b[38;5;241m=\u001b[39mTRAINING_IMAGE,\n\u001b[1;32m     43\u001b[0m         command\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m         ],\n\u001b[1;32m     61\u001b[0m     )\n\u001b[1;32m     64\u001b[0m \u001b[38;5;129;43m@dsl\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxgboost-train-pipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mpipeline\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_id\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVERTEX_PROJECT_ID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_location\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVERTEX_LOCATION\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mingestion_project_id\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVERTEX_PROJECT_ID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimple_xgboost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_id\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpreprocessing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_location\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVERTEX_LOCATION\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mingestion_dataset_id\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchicago_taxi_trips\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2022-12-01 00:00:00\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresource_suffix\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRESOURCE_SUFFIX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataset_uri\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"\u001b[39;49;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;43;03m    XGB training pipeline which:\u001b[39;49;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;43;03m     1. Splits and extracts a dataset from BQ to GCS\u001b[39;49;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;43;03m        test_dataset_uri (str): Optional. GCS URI of statis held-out test dataset.\u001b[39;49;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;43;03m    \"\"\"\u001b[39;49;00m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Create variables to ensure the same arguments are passed\u001b[39;49;00m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# into different components of the pipeline\u001b[39;49;00m\n",
      "File \u001b[0;32m~/dev/vertex-pipelines-end-to-end-samples/pipelines/.venv/lib/python3.9/site-packages/kfp/components/pipeline_context.py:65\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(func, name, description, pipeline_root, display_name)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pipeline_root:\n\u001b[1;32m     63\u001b[0m     func\u001b[38;5;241m.\u001b[39mpipeline_root \u001b[38;5;241m=\u001b[39m pipeline_root\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomponent_factory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_graph_component_from_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplay_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/vertex-pipelines-end-to-end-samples/pipelines/.venv/lib/python3.9/site-packages/kfp/components/component_factory.py:616\u001b[0m, in \u001b[0;36mcreate_graph_component_from_func\u001b[0;34m(func, name, description, display_name)\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Implementation for the @pipeline decorator.\u001b[39;00m\n\u001b[1;32m    606\u001b[0m \n\u001b[1;32m    607\u001b[0m \u001b[38;5;124;03mThe decorator is defined under pipeline_context.py. See the\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;124;03mdecorator for the canonical documentation for this function.\u001b[39;00m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    611\u001b[0m component_spec \u001b[38;5;241m=\u001b[39m extract_component_interface(\n\u001b[1;32m    612\u001b[0m     func,\n\u001b[1;32m    613\u001b[0m     description\u001b[38;5;241m=\u001b[39mdescription,\n\u001b[1;32m    614\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    615\u001b[0m )\n\u001b[0;32m--> 616\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_component\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGraphComponent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomponent_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomponent_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplay_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/vertex-pipelines-end-to-end-samples/pipelines/.venv/lib/python3.9/site-packages/kfp/components/graph_component.py:58\u001b[0m, in \u001b[0;36mGraphComponent.__init__\u001b[0;34m(self, component_spec, pipeline_func, display_name)\u001b[0m\n\u001b[1;32m     49\u001b[0m     args_list\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     50\u001b[0m         pipeline_channel\u001b[38;5;241m.\u001b[39mcreate_pipeline_channel(\n\u001b[1;32m     51\u001b[0m             name\u001b[38;5;241m=\u001b[39marg_name,\n\u001b[1;32m     52\u001b[0m             channel_type\u001b[38;5;241m=\u001b[39minput_spec\u001b[38;5;241m.\u001b[39mtype,\n\u001b[1;32m     53\u001b[0m             is_artifact_list\u001b[38;5;241m=\u001b[39minput_spec\u001b[38;5;241m.\u001b[39mis_artifact_list,\n\u001b[1;32m     54\u001b[0m         ))\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pipeline_context\u001b[38;5;241m.\u001b[39mPipeline(\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponent_spec\u001b[38;5;241m.\u001b[39mname) \u001b[38;5;28;01mas\u001b[39;00m dsl_pipeline:\n\u001b[0;32m---> 58\u001b[0m     pipeline_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dsl_pipeline\u001b[38;5;241m.\u001b[39mtasks:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTask is missing from pipeline.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 128\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(project_id, project_location, ingestion_project_id, model_name, dataset_id, dataset_location, ingestion_dataset_id, timestamp, resource_suffix, test_dataset_uri)\u001b[0m\n\u001b[1;32m    114\u001b[0m hparams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    115\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m    116\u001b[0m     early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m     label\u001b[38;5;241m=\u001b[39mlabel_column_name,\n\u001b[1;32m    123\u001b[0m )\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# generate sql queries which are used in ingestion and preprocessing\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# operations\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m queries_folder \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(\u001b[38;5;18;43m__file__\u001b[39;49m)\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqueries\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m preprocessing_query \u001b[38;5;241m=\u001b[39m generate_query(\n\u001b[1;32m    131\u001b[0m     queries_folder \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessing.sql\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    132\u001b[0m     source_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mingestion_project_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mingestion_dataset_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m     test_table\u001b[38;5;241m=\u001b[39mtest_table,\n\u001b[1;32m    143\u001b[0m )\n\u001b[1;32m    145\u001b[0m preprocessing \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    146\u001b[0m     BigqueryQueryJobOp(\n\u001b[1;32m    147\u001b[0m         project\u001b[38;5;241m=\u001b[39mproject_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;241m.\u001b[39mset_display_name(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIngest & preprocess data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    153\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "from google_cloud_pipeline_components.v1.bigquery import BigqueryQueryJobOp\n",
    "from kfp import dsl\n",
    "from kfp.dsl import Dataset, Input, Metrics, Model, Output, OutputPath\n",
    "from pipelines import generate_query\n",
    "from bigquery_components import extract_bq_to_dataset\n",
    "from vertex_components import upload_model\n",
    "\n",
    "CONTAINER_IMAGE_REGISTRY = os.environ[\"CONTAINER_IMAGE_REGISTRY\"]\n",
    "RESOURCE_SUFFIX = os.environ.get(\"RESOURCE_SUFFIX\", \"default\")\n",
    "TRAINING_IMAGE = f\"{CONTAINER_IMAGE_REGISTRY}/training:{RESOURCE_SUFFIX}\"\n",
    "SERVING_IMAGE = f\"{CONTAINER_IMAGE_REGISTRY}/serving:{RESOURCE_SUFFIX}\"\n",
    "\n",
    "\n",
    "@dsl.container_component\n",
    "def train(\n",
    "    train_data: Input[Dataset],\n",
    "    valid_data: Input[Dataset],\n",
    "    test_data: Input[Dataset],\n",
    "    model: Output[Model],\n",
    "    model_output_uri: OutputPath(str),\n",
    "    metrics: Output[Metrics],\n",
    "    hparams: dict,\n",
    "):\n",
    "    return dsl.ContainerSpec(\n",
    "        image=TRAINING_IMAGE,\n",
    "        command=[\"python\"],\n",
    "        args=[\n",
    "            \"training/train.py\",\n",
    "            \"--train-data\",\n",
    "            train_data.path,\n",
    "            \"--valid-data\",\n",
    "            valid_data.path,\n",
    "            \"--test-data\",\n",
    "            test_data.path,\n",
    "            \"--model\",\n",
    "            model.path,\n",
    "            \"--model-output-uri\",\n",
    "            model_output_uri,\n",
    "            \"--metrics\",\n",
    "            metrics.path,\n",
    "            \"--hparams\",\n",
    "            hparams,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "@dsl.pipeline(name=\"xgboost-train-pipeline\")\n",
    "def pipeline(\n",
    "    project_id: str = os.environ.get(\"VERTEX_PROJECT_ID\"),\n",
    "    project_location: str = os.environ.get(\"VERTEX_LOCATION\"),\n",
    "    ingestion_project_id: str = os.environ.get(\"VERTEX_PROJECT_ID\"),\n",
    "    model_name: str = \"simple_xgboost\",\n",
    "    dataset_id: str = \"preprocessing\",\n",
    "    dataset_location: str = os.environ.get(\"VERTEX_LOCATION\"),\n",
    "    ingestion_dataset_id: str = \"chicago_taxi_trips\",\n",
    "    timestamp: str = \"2022-12-01 00:00:00\",\n",
    "    resource_suffix: str = os.environ.get(\"RESOURCE_SUFFIX\"),\n",
    "    test_dataset_uri: str = \"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    XGB training pipeline which:\n",
    "     1. Splits and extracts a dataset from BQ to GCS\n",
    "     2. Trains a model via Vertex AI CustomTrainingJob\n",
    "     3. Evaluates the model against the current champion model\n",
    "     4. If better the model becomes the new default model\n",
    "\n",
    "    Args:\n",
    "        project_id (str): project id of the Google Cloud project\n",
    "        project_location (str): location of the Google Cloud project\n",
    "        ingestion_project_id (str): project id containing the source bigquery data\n",
    "            for ingestion. This can be the same as `project_id` if the source data is\n",
    "            in the same project where the ML pipeline is executed.\n",
    "        model_name (str): name of model\n",
    "        dataset_id (str): id of BQ dataset used to store all staging data & predictions\n",
    "        dataset_location (str): location of dataset\n",
    "        ingestion_dataset_id (str): dataset id of ingestion data\n",
    "        timestamp (str): Optional. Empty or a specific timestamp in ISO 8601 format\n",
    "            (YYYY-MM-DDThh:mm:ss.sssÂ±hh:mm or YYYY-MM-DDThh:mm:ss).\n",
    "            If any time part is missing, it will be regarded as zero.\n",
    "        resource_suffix (str): Optional. Additional suffix to append GCS resources\n",
    "            that get overwritten.\n",
    "        test_dataset_uri (str): Optional. GCS URI of statis held-out test dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create variables to ensure the same arguments are passed\n",
    "    # into different components of the pipeline\n",
    "    label_column_name = \"total_fare\"\n",
    "    time_column = \"trip_start_timestamp\"\n",
    "    ingestion_table = \"taxi_trips\"\n",
    "    table_suffix = f\"_xgb_training_{resource_suffix}\"  # suffix to table names\n",
    "    ingested_table = \"ingested_data\" + table_suffix\n",
    "    preprocessed_table = \"preprocessed_data\" + table_suffix\n",
    "    train_table = \"train_data\" + table_suffix\n",
    "    valid_table = \"valid_data\" + table_suffix\n",
    "    test_table = \"test_data\" + table_suffix\n",
    "    primary_metric = \"rootMeanSquaredError\"\n",
    "    hparams = dict(\n",
    "        n_estimators=200,\n",
    "        early_stopping_rounds=10,\n",
    "        objective=\"reg:squarederror\",\n",
    "        booster=\"gbtree\",\n",
    "        learning_rate=0.3,\n",
    "        min_split_loss=0,\n",
    "        max_depth=6,\n",
    "        label=label_column_name,\n",
    "    )\n",
    "\n",
    "    # generate sql queries which are used in ingestion and preprocessing\n",
    "    # operations\n",
    "\n",
    "    queries_folder = pathlib.Path(__file__).parent / \"queries\"\n",
    "\n",
    "    preprocessing_query = generate_query(\n",
    "        queries_folder / \"preprocessing.sql\",\n",
    "        source_dataset=f\"{ingestion_project_id}.{ingestion_dataset_id}\",\n",
    "        source_table=ingestion_table,\n",
    "        preprocessing_dataset=f\"{ingestion_project_id}.{dataset_id}\",\n",
    "        ingested_table=ingested_table,\n",
    "        dataset_region=project_location,\n",
    "        filter_column=time_column,\n",
    "        target_column=label_column_name,\n",
    "        filter_start_value=timestamp,\n",
    "        train_table=train_table,\n",
    "        validation_table=valid_table,\n",
    "        test_table=test_table,\n",
    "    )\n",
    "\n",
    "    preprocessing = (\n",
    "        BigqueryQueryJobOp(\n",
    "            project=project_id,\n",
    "            location=dataset_location,\n",
    "            query=preprocessing_query,\n",
    "        )\n",
    "        .set_caching_options(False)\n",
    "        .set_display_name(\"Ingest & preprocess data\")\n",
    "    )\n",
    "\n",
    "    # data extraction to gcs\n",
    "\n",
    "    train_dataset = (\n",
    "        extract_bq_to_dataset(\n",
    "            bq_client_project_id=project_id,\n",
    "            source_project_id=project_id,\n",
    "            dataset_id=dataset_id,\n",
    "            table_name=train_table,\n",
    "            dataset_location=dataset_location,\n",
    "        )\n",
    "        .after(preprocessing)\n",
    "        .set_display_name(\"Extract train data\")\n",
    "        .set_caching_options(False)\n",
    "    ).outputs[\"dataset\"]\n",
    "    valid_dataset = (\n",
    "        extract_bq_to_dataset(\n",
    "            bq_client_project_id=project_id,\n",
    "            source_project_id=project_id,\n",
    "            dataset_id=dataset_id,\n",
    "            table_name=valid_table,\n",
    "            dataset_location=dataset_location,\n",
    "        )\n",
    "        .after(preprocessing)\n",
    "        .set_display_name(\"Extract validation data\")\n",
    "        .set_caching_options(False)\n",
    "    ).outputs[\"dataset\"]\n",
    "    test_dataset = (\n",
    "        extract_bq_to_dataset(\n",
    "            bq_client_project_id=project_id,\n",
    "            source_project_id=project_id,\n",
    "            dataset_id=dataset_id,\n",
    "            table_name=test_table,\n",
    "            dataset_location=dataset_location,\n",
    "            destination_gcs_uri=test_dataset_uri,\n",
    "        )\n",
    "        .after(preprocessing)\n",
    "        .set_display_name(\"Extract test data\")\n",
    "        .set_caching_options(False)\n",
    "    ).outputs[\"dataset\"]\n",
    "\n",
    "    train_model = train(\n",
    "        train_data=train_dataset,\n",
    "        valid_data=valid_dataset,\n",
    "        test_data=test_dataset,\n",
    "        hparams=hparams,\n",
    "    ).set_display_name(\"Train model\")\n",
    "\n",
    "    upload_model_op = upload_model(\n",
    "        project_id=project_id,\n",
    "        project_location=project_location,\n",
    "        model=train_model.outputs[\"model\"],\n",
    "        model_evaluation=train_model.outputs[\"metrics\"],\n",
    "        test_dataset=test_dataset,\n",
    "        eval_metric=primary_metric,\n",
    "        eval_lower_is_better=True,\n",
    "        serving_container_image=SERVING_IMAGE,\n",
    "        model_name=model_name,\n",
    "        pipeline_job_id=\"{{$.pipeline_job_name}}\",\n",
    "    ).set_display_name(\"Upload model\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
