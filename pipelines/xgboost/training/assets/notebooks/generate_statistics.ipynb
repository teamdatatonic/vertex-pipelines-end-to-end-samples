{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate tfdv statistics given a Dataset as input.\n",
    "\n",
    "Wraps `tfdv.generate_statistics` function.\n",
    "\n",
    "## Args:\n",
    "- statistics (Output[Artifact]): this parameter will be passed automatically by the KFP orchestrator at runtime.\n",
    "- dataset (Input[Dataset]): the desired Input[Dataset] from which the statistics will need to be generated.\n",
    "- use_dataflow (bool): whether to run the job using Dataflow instead of locally. Defaults to False.\n",
    "- project_id (str): Google Cloud project ID (for use with Dataflow)\n",
    "- region (str): Region in which to run the Dataflow job\n",
    "- subnetwork (str): Subnetwork in which to run the Dataflow job, in the form regions/<REGION>/subnetworks/<SUBNET_NAME>. Dataflow uses the project default network by default.\n",
    "- use_public_ips (bool): Whether the Dataflow worker nodes should have public IP addresses. Defaults to True.\n",
    "- tfdv_container_image (str): URI of a container image to use for the Dataflow workers. It should be based on the appropriate Apache Beam base image (Python 3.7, >=v2.35.0), and should have TFDV preinstalled (same version as is used here). An example Dockerfile can be found in this repo under containers/tfdv. If not provided, Dataflow will install from PyPi.\n",
    "- gcs_staging_location (str): GCS path for a Dataflow staging location.\n",
    "- gcs_temp_location (str): GCS path for a Dataflow temp/scratch location.\n",
    "- extra_standard_options (dict): any extra StandardOptions you want to use for the Beam job. Note that these are applied last, so may overwrite any of the settings applied by this function. See the reference here: https://beam.apache.org/releases/pydoc/current/_modules/apache_beam/options/pipeline_options.html#StandardOptions\n",
    "- extra_setup_options (dict): any extra SetupOptions you want to use for the Beam job. Note that these are applied last, so may overwrite any of the settings applied by this function. See the reference here: https://beam.apache.org/releases/pydoc/current/_modules/apache_beam/options/pipeline_options.html#SetupOptions\n",
    "- extra_worker_options (dict): any extra WorkerOptions you want to use for the Beam job. Note that these are applied last, so may overwrite any of the settings applied by this function. See the reference here: https://beam.apache.org/releases/pydoc/current/_modules/apache_beam/options/pipeline_options.html#WorkerOptions\n",
    "- extra_google_cloud_options (dict): any extra GoogleCloudOptions you want to use for the Beam job. Note that these are applied last, so may overwrite any of the settings applied by this function. See the reference here: https://beam.apache.org/releases/pydoc/current/_modules/apache_beam/options/pipeline_options.html#GoogleCloudOptions\n",
    "- extra_debug_options (dict): any extra DebugOptions you want to use for the Beam job. Note that these are applied last, so may overwrite any of the settings applied by this function. See the reference here: https://beam.apache.org/releases/pydoc/current/_modules/apache_beam/options/pipeline_options.html#DebugOptions\n",
    "- file_pattern (str): Read data from one or more files. If empty, then input data is read from single file. For multiple files, use a pattern e.g. \"file-*.csv\".\n",
    "- tfdv_stats_options (dict): Options for generating statistics. Can pass pre-defined schema, sampling rate, histogram buckets, allowlist for features etc as part of these options. See reference here: https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/StatsOptions\n",
    "\n",
    "\n",
    "## Outputs:\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "statistics_uri: str = \"\"\n",
    "dataset_uri: str = \"\"\n",
    "use_dataflow: bool = False\n",
    "project_id: str = None\n",
    "region: str = None\n",
    "subnetwork: str = None\n",
    "use_public_ips: bool = True\n",
    "tfdv_container_image: str = None\n",
    "gcs_staging_location: str = None\n",
    "gcs_temp_location: str = None\n",
    "extra_standard_options: dict = {}\n",
    "extra_setup_options: dict = {}\n",
    "extra_worker_options: dict = {}\n",
    "extra_google_cloud_options: dict = {}\n",
    "extra_debug_options: dict = {}\n",
    "file_pattern: str = None\n",
    "tfdv_stats_options: dict = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import logging\n",
    "import os\n",
    "import tensorflow_data_validation as tfdv\n",
    "from apache_beam.options.pipeline_options import (\n",
    "    PipelineOptions,\n",
    "    GoogleCloudOptions,\n",
    "    StandardOptions,\n",
    "    SetupOptions,\n",
    "    WorkerOptions,\n",
    "    DebugOptions,\n",
    ")\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "def write_setup_py_file():\n",
    "    \"\"\"Writes the required setup.py file to disk, ready for use by TFDV\"\"\"\n",
    "\n",
    "    setup_file_contents = inspect.cleandoc(\n",
    "        f\"\"\"\n",
    "            import setuptools\n",
    "            setuptools.setup(\n",
    "                install_requires=['tensorflow-data-validation=={tfdv.__version__}'],\n",
    "                packages=setuptools.find_packages()\n",
    "            )\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    # Create the setup.py file for managing pipeline dependencies\n",
    "    logging.info(\"Writing setup.py to disk\")\n",
    "    with open(\"./setup.py\", \"w\") as f:\n",
    "        f.write(setup_file_contents)\n",
    "\n",
    "pipeline_options = PipelineOptions()\n",
    "debug_options = pipeline_options.view_as(DebugOptions)\n",
    "google_cloud_options = pipeline_options.view_as(GoogleCloudOptions)\n",
    "setup_options = pipeline_options.view_as(SetupOptions)\n",
    "standard_options = pipeline_options.view_as(StandardOptions)\n",
    "worker_options = pipeline_options.view_as(WorkerOptions)\n",
    "\n",
    "if use_dataflow:\n",
    "\n",
    "    # Set beam_runner to use Dataflow\n",
    "    logging.info(f\"Using Beam Runner: DataflowRunner\")\n",
    "    standard_options.runner = \"DataflowRunner\"\n",
    "\n",
    "    # Set Google Cloud options\n",
    "    if not project_id:\n",
    "        raise ValueError(\"You must provide project_id in order to use DataFlow\")\n",
    "    logging.info(f\"GCP Project ID: {project_id}\")\n",
    "    google_cloud_options.project = project_id\n",
    "\n",
    "    if not region:\n",
    "        raise ValueError(\"You must provide region in order to use DataFlow\")\n",
    "    logging.info(f\"GCP Region: {region}\")\n",
    "    google_cloud_options.region = region\n",
    "\n",
    "    if not gcs_staging_location:\n",
    "        raise ValueError(\n",
    "            \"You must provide gcs_staging_location in order to use DataFlow\"\n",
    "        )\n",
    "    logging.info(f\"GCS staging location: {gcs_staging_location}\")\n",
    "    google_cloud_options.staging_location = gcs_staging_location\n",
    "\n",
    "    if not gcs_temp_location:\n",
    "        raise ValueError(\n",
    "            \"You must provide gcs_temp_location in order to use DataFlow\"\n",
    "        )\n",
    "    logging.info(f\"GCS temp location: {gcs_temp_location}\")\n",
    "    google_cloud_options.temp_location = gcs_temp_location\n",
    "\n",
    "    # Set Worker options\n",
    "    use_public_ips = bool(use_public_ips)  # cast to bool to be sure\n",
    "    logging.info(f\"Dataflow using public IP addresses: {use_public_ips}\")\n",
    "    worker_options.use_public_ips = use_public_ips\n",
    "    if subnetwork:\n",
    "        logging.info(f\"Dataflow subnetwork: {subnetwork}\")\n",
    "        worker_options.subnetwork = subnetwork\n",
    "\n",
    "    # If using a prebaked TFDV+Beam container image, set these options\n",
    "    if tfdv_container_image:\n",
    "        logging.info(f\"Custom Dataflow container: {tfdv_container_image}\")\n",
    "        logging.info(\"Using Dataflow v2 runner\")\n",
    "        debug_options.add_experiment(\"use_runner_v2\")\n",
    "        setup_options.sdk_location = \"container\"\n",
    "        worker_options.sdk_container_image = tfdv_container_image\n",
    "    # If not using a prebaked container image, use setup.py for TFDV installation\n",
    "    else:\n",
    "        logging.info(f\"Using setup.py file. TFDV version is {tfdv.__version__}\")\n",
    "        write_setup_py_file()\n",
    "        setup_options.setup_file = \"./setup.py\"\n",
    "\n",
    "else:\n",
    "    # If not using Dataflow, use DirectRunner\n",
    "    logging.info(f\"Using Beam Runner: DirectRunner\")\n",
    "    standard_options.runner = \"DirectRunner\"\n",
    "\n",
    "# Apply any extra pipeline options provided by the user\n",
    "\n",
    "for key, val in extra_standard_options.items():\n",
    "    setattr(standard_options, key, val)\n",
    "\n",
    "for key, val in extra_setup_options.items():\n",
    "    setattr(setup_options, key, val)\n",
    "\n",
    "for key, val in extra_google_cloud_options.items():\n",
    "    setattr(google_cloud_options, key, val)\n",
    "\n",
    "for key, val in extra_worker_options.items():\n",
    "    setattr(worker_options, key, val)\n",
    "\n",
    "for key, val in extra_debug_options.items():\n",
    "    setattr(debug_options, key, val)\n",
    "\n",
    "# if file_pattern is provided, join dataset.uri with file_pattern\n",
    "if file_pattern:\n",
    "    dataset_uri = os.path.join(dataset.uri, file_pattern)\n",
    "\n",
    "# if stats options are provided, pass those to generate_statistics_from_csv\n",
    "stats_options = tfdv.StatsOptions()\n",
    "if tfdv_stats_options:\n",
    "    # if schema is provided, load and pass to stats_options dict\n",
    "    if \"schema\" in tfdv_stats_options:\n",
    "        tfdv_stats_options[\"schema\"] = tfdv.load_schema_text(\n",
    "            tfdv_stats_options[\"schema\"]\n",
    "        )\n",
    "    stats_options = tfdv.StatsOptions(**tfdv_stats_options)\n",
    "\n",
    "logging.info(f\"Generating statistics from: {dataset_uri}\")\n",
    "logging.info(f\"Saving statistics to: {statistics_uri}\")\n",
    "\n",
    "tfdv.generate_statistics_from_csv(\n",
    "    data_location=dataset_uri,\n",
    "    output_path=statistics_uri,\n",
    "    pipeline_options=pipeline_options,\n",
    "    stats_options=stats_options,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9082739a6d7aef52dd1add0dde88e50ad5a15a02f84caf2b0de4c075d2416be6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
