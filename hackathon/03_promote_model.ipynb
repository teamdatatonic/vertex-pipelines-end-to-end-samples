{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# MLOps Hackathon\n",
    "\n",
    "This hackathon is based on the [open-source Turbo Template](https://github.com/teamdatatonic/vertex-pipelines-end-to-end-samples).\n",
    "Through this notebook series you'll get hands-on with the template and Google Cloud.\n",
    "The hackathon is structured into the following exercises:\n",
    "\n",
    "1. [Health check](./01_health_check.ipynb)\n",
    "1. [Run pipelines](./02_run_pipelines.ipynb)\n",
    "1. **[Promote model](./03_promote_model.ipynb) - this notebook**\n",
    "1. [Challenge: Model monitoring](./04_monitoring_challenge.ipynb)\n",
    "1. [Challenge: Real-time predictions](./05_realtime_challenge.ipynb)\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook you'll use a recommended a Google Cloud architecture to promote your model from development to production.\n",
    "You'll learn about pull requests, releases, Cloud Build, and Cloud Scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticate\n",
    "\n",
    "Set your project ID and authenticate using your Google Account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERTEX_PROJECT_ID = \"my-project-id\"\n",
    "GOOGLE_ACCOUNT = \"user@company.com\"\n",
    "! gcloud config set project {VERTEX_PROJECT_ID}\n",
    "! gcloud config set account {GOOGLE_ACCOUNT}\n",
    "! gcloud auth login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/mlops-hackathon\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Staging: Test your recent changes\n",
    "\n",
    "Remember the updated training container from the last notebook? Let's commit your changes!\n",
    "\n",
    "**‚û°Ô∏è Exercise:** Submit your code changes:\n",
    "1. Create a new branch `feat/new-model-<your name>`\n",
    "1. Commit, go to github and open a pull request from your new branch to `develop` branch of the **same** repo\n",
    "1. Add a new comment with the content `/gcbrun`\n",
    "1. Don't merge your pull request, however, observe what's triggered by your comment\n",
    "\n",
    "**‚û°Ô∏è Exercise:** Check your understanding:\n",
    "- Do you know where the executed jobs are defined? (Hint: you're running pipelines in Cloud Build)\n",
    "- In which Google Cloud project are the jobs executed? Do you know why this decision was made?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production: Promote your model\n",
    "\n",
    "Let's recall this Google Cloud architecture:\n",
    "\n",
    "![Architecture](../docs/images/architecture.png)\n",
    "\n",
    "**‚û°Ô∏è Exercise:** \n",
    "\n",
    "1. Locate your production project in Google Cloud console\n",
    "1. Navigate to Cloud Build and verify that it includes a trigger upon tag\n",
    "1. Go to [the hackathon repository](https://github.com/teamdatatonic/mlops-hackathon)\n",
    "1. Create a release from your branch and provide the tag name `v1.0.0-<your name>`\u001b\n",
    "\n",
    "![Create release](./images/create_release.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚û°Ô∏è Exercise:** Go back to Cloud Build in the **production** Google Cloud project and monitor the new job. What's happening in the job? Once finished:\n",
    "\n",
    "1. Go to Artifact Registry in the Google Cloud console and locate the new docker images and pipelines\n",
    "2. Can you find the created artifacts by your release?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production: Schedule your pipeline\n",
    "\n",
    "After successfully creating a release and thereby promoting your code and containers to the **production** environment, let's schedule your training pipeline so that you can continuously retrain your model whenever new training data becomes available.\n",
    "\n",
    "Update the variables below before executing the next code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tag = \"v1.0.0-eugene\"\n",
    "location = \"europe-west2\"\n",
    "project = \"dt-sky-mlops-hackathon-prod\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"template_path\": \"https://europe-west2-kfp.pkg.dev/dt-sky-mlops-hackathon-prod/vertex-pipelines/turbo-training-pipeline/v1.0.0-eugene\", \"display_name\": \"turbo-training-pipeline-v1.0.0-eugene\", \"enable_caching\": \"false\", \"pipeline_parameters\": {\"project\": \"dt-sky-mlops-hackathon-prod\", \"location\": \"europe-west2\", \"bq_location\": \"US\", \"bq_source_uri\": \"bigquery-public-data.chicago_taxi_trips.taxi_trips\", \"model_name\": \"model-v1.0.0-eugene\"}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "template_path = f\"https://{location}-kfp.pkg.dev/{project}/vertex-pipelines/turbo-training-pipeline/{tag}\"\n",
    "display_name = f\"turbo-training-pipeline-{tag}\"\n",
    "enable_caching = \"false\"\n",
    "\n",
    "data = dict(\n",
    "    template_path       = template_path,\n",
    "    display_name        = display_name,\n",
    "    enable_caching      = enable_caching,\n",
    "    pipeline_parameters = dict(\n",
    "        project = project,\n",
    "        location = location,\n",
    "        bq_location = \"US\",\n",
    "        bq_source_uri = \"bigquery-public-data.chicago_taxi_trips.taxi_trips\",\n",
    "        model_name = f\"model-{tag}\",\n",
    "    )\n",
    ")\n",
    "\n",
    "print(json.dumps(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚û°Ô∏è Exercise:** Schedule the training pipeline using Cloud Scheduler:\n",
    "\n",
    "1. Locate Cloud Scheduler in the **production** Google Cloud project and create a new job <p align=\"center\">\n",
    "  <img src=\"./images/scheduler_1.png\" width=\"400\"/>\n",
    "</p>\n",
    "2. Paste as the message body the generated JSON from above <p align=\"center\">\n",
    "  <img src=\"./images/scheduler_2.png\" width=\"400\"/>\n",
    "</p>\n",
    "3. Create the job and force a manual execution\n",
    "4. Go to Vertex AI pipeline and observe whether your training pipeline is running\n",
    "\n",
    "**‚û°Ô∏è Exercise:** Can you think about a better (e.g. automated) way of scheduling pipelines?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've successfully tested, promoted, and scheduled your first pipeline in a production environment! üéâ Now you're ready for the next exercise!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "bb5c7b0035bb37e2e2e56e6840dfdd8f7fa070884ae8e041fbcae450545b1006"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
